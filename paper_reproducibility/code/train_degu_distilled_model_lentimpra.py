#!/usr/bin/env python3
"""
DEGU Distillation Training Script for lentiMPRA DREAM-RNN Models

This script implements the DEGU (Distilling Ensembles for Genomic Uncertainty-aware models) 
methodology as described in the DEGU paper for lentiMPRA data. It trains distilled student models using:

1. Ensemble mean predictions as targets for the main prediction task (activity)
2. Ensemble standard deviation as targets for epistemic uncertainty estimation
3. Aleatoric uncertainty from experimental replicates (when available)
4. Multitask learning with MSE loss for all tasks
5. Same DREAM-RNN architecture as teacher models but with expanded output heads

The script loads ensemble predictions generated by evaluate_ensemble_and_generate_distillation_data.py
and trains distilled models that approximate the ensemble's function while providing
uncertainty estimates.

Based on the @DREAM_paper architecture and @DEGU_paper methodology.
"""

import argparse
import os
import sys
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from pathlib import Path
import random
from typing import Dict, Tuple, Optional
import logging
import h5py

# Disable cuDNN early to avoid version conflicts
import torch
if torch.cuda.is_available():
    torch.backends.cudnn.enabled = False
    print("Disabled cuDNN to avoid version conflicts")

# Import the model from model_zoo to use config-driven heads
from model_zoo import DREAM_RNN_lentiMPRA

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DEGULentiMPRADistillationTrainer:
    """
    DEGU Distillation Trainer for lentiMPRA DREAM-RNN Models
    
    This class implements the DEGU methodology for lentiMPRA data, training student models
    to approximate ensemble predictions while providing uncertainty estimates.
    """
    
    def __init__(self, 
                 distillation_data_path: str,
                 output_dir: str,
                 model_index: int = 0,
                 batch_size: int = 1024,
                 learning_rate: float = 0.005,
                 epochs: int = 80,
                 device: str = 'cuda',
                 config: dict = None):
        """
        Initialize DEGU distillation trainer for lentiMPRA data.
        
        Args:
            distillation_data_path: Path to .npz file containing ensemble predictions
            output_dir: Directory to save trained model
            model_index: Index for this model (used for random seed)
            batch_size: Batch size for training
            learning_rate: Learning rate for optimizer
            epochs: Maximum number of training epochs
            device: Device to use for training ('cuda' or 'cpu')
        """
        self.distillation_data_path = Path(distillation_data_path)
        self.output_dir = Path(output_dir)
        self.model_index = model_index
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.device = device
        self.config = config or {}
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Set random seeds for reproducibility (different but consistent per model)
        self.seed = 42 + self.model_index * 1000
        random.seed(self.seed)
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(self.seed)
            torch.cuda.manual_seed_all(self.seed)
        
        # Initialize model and training components
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        
        # Data attributes
        self.train_sequences = None
        self.val_sequences = None
        self.test_sequences = None
        self.train_mean = None
        self.train_std = None
        self.val_mean = None
        self.val_std = None
        self.test_mean = None
        self.test_std = None
        self.train_aleatoric = None
        self.val_aleatoric = None
        self.test_aleatoric = None
        
        # Sequence length for lentiMPRA (230bp)
        self.seq_len = 230
        
        print(f"Initialized DEGU lentiMPRA distillation trainer")
        print(f"Device: {self.device}")
        print(f"Model index: {self.model_index}")
        print(f"Random seed: {self.seed}")
    
    def _load_distillation_data(self):
        """Load ensemble predictions and actual sequences for lentiMPRA data."""
        print(f"Loading distillation data from: {self.distillation_data_path}")
        
        # Check if it's an HDF5 file (for 1.0 downsample ratio) or NPZ file
        if str(self.distillation_data_path).endswith('.h5'):
            # Load from HDF5 file (for 1.0 downsample ratio)
            import h5py
            with h5py.File(self.distillation_data_path, 'r') as f:
                # Load ensemble mean and std predictions (keep on CPU to avoid memory issues)
                self.train_mean = torch.FloatTensor(f['train_mean'][:])  # Keep on CPU
                self.train_std = torch.FloatTensor(f['train_std'][:])    # Keep on CPU
                self.val_mean = torch.FloatTensor(f['val_mean'][:])      # Keep on CPU
                self.val_std = torch.FloatTensor(f['val_std'][:])        # Keep on CPU
                self.test_mean = torch.FloatTensor(f['test_mean'][:])    # Keep on CPU
                self.test_std = torch.FloatTensor(f['test_std'][:])      # Keep on CPU
        else:
            # Load from NPZ file (for other downsample ratios)
            data = np.load(self.distillation_data_path)
            
            # Load ensemble mean and std predictions (keep on CPU to avoid memory issues)
            self.train_mean = torch.FloatTensor(data['train_mean'])  # Keep on CPU
            self.train_std = torch.FloatTensor(data['train_std'])    # Keep on CPU
            self.val_mean = torch.FloatTensor(data['val_mean'])      # Keep on CPU
            self.val_std = torch.FloatTensor(data['val_std'])        # Keep on CPU
            self.test_mean = torch.FloatTensor(data['test_mean'])    # Keep on CPU
            self.test_std = torch.FloatTensor(data['test_std'])      # Keep on CPU
        
        # Load aleatoric uncertainty if available (keep on CPU)
        if str(self.distillation_data_path).endswith('.h5'):
            # Check for aleatoric data in HDF5 file
            with h5py.File(self.distillation_data_path, 'r') as f:
                if 'train_aleatoric' in f:
                    self.train_aleatoric = torch.FloatTensor(f['train_aleatoric'][:])  # Keep on CPU
                    self.val_aleatoric = torch.FloatTensor(f['val_aleatoric'][:])      # Keep on CPU
                else:
                    self.train_aleatoric = None
                    self.val_aleatoric = None
        else:
            # Check for aleatoric data in NPZ file
            if 'train_aleatoric' in data:
                self.train_aleatoric = torch.FloatTensor(data['train_aleatoric'])  # Keep on CPU
                self.val_aleatoric = torch.FloatTensor(data['val_aleatoric'])      # Keep on CPU
            else:
                self.train_aleatoric = None
                self.val_aleatoric = None
        
        # Set test aleatoric to None for now (not used in training)
        self.test_aleatoric = None
        
        if self.train_aleatoric is not None:
            print("Loaded aleatoric uncertainty data")
        else:
            print("No aleatoric uncertainty data found - will train without it")
        
        print(f"Training data shape: {self.train_mean.shape}")
        print(f"Validation data shape: {self.val_mean.shape}")
        print(f"Test data shape: {self.test_mean.shape}")
        
        # Load actual DNA sequences
        self._load_actual_sequences()
    
    def _load_actual_sequences(self):
        """Load actual DNA sequences and original experimental data from the corresponding lentiMPRA .h5 file."""
        # Determine the cell type and downsample ratio from the ensemble data path
        path_str = str(self.distillation_data_path)
        
        # Extract cell type
        if "K562" in path_str:
            celltype = "K562"
        elif "HepG2" in path_str:
            celltype = "HepG2"
        else:
            raise ValueError("Could not determine cell type from path")
        
        # Extract downsample ratio
        if "0.005" in path_str:
            downsample_ratio = "0.005"
        elif "0.1" in path_str:
            downsample_ratio = "0.1"
        elif "0.25" in path_str:
            downsample_ratio = "0.25"
        elif "0.5" in path_str:
            downsample_ratio = "0.5"
        elif "0.75" in path_str:
            downsample_ratio = "0.75"
        else:
            downsample_ratio = "1.0"
        
        # Load the corresponding lentiMPRA data file
        # Get the repo root directory (go up from paper_reproducibility/code/)
        repo_root = Path(__file__).resolve().parent.parent.parent
        data_file = repo_root / f"zenodo/data/lentiMPRA_{celltype}_activity_and_aleatoric_data.h5"
        logger.info(f"Loading actual sequences and experimental data from: {data_file}")
        
        import h5py
        with h5py.File(data_file, 'r') as f:
            train_sequences = f['Train/X'][:]
            val_sequences = f['Val/X'][:]
            test_sequences = f['Test/X'][:]
            
            # Load original experimental data for evaluation
            train_experimental = f['Train/y'][:]
            val_experimental = f['Val/y'][:]
            test_experimental = f['Test/y'][:]
        
        # Apply on-the-fly downsampling to training data if needed (same as standard training script)
        if downsample_ratio != "1.0":
            downsample_float = float(downsample_ratio)
            if downsample_float < 1.0:
                logger.info(f"Downsampling training sequences to {downsample_float:.1%} with fixed seed for reproducibility")
                rng = np.random.default_rng(1234)  # Same fixed seed as standard training script
                n_samples = max(1, int(len(train_sequences) * downsample_float))
                indices = rng.choice(len(train_sequences), n_samples, replace=False)
                train_sequences = train_sequences[indices]
                train_experimental = train_experimental[indices]
                logger.info(f"  Training sequences downsampled from {int(len(train_sequences)/downsample_float)} to {len(train_sequences)} samples")
        
        # Apply downsampling if specified (with fixed seed for reproducibility)
        if downsample_ratio != "1.0":
            downsample_float = float(downsample_ratio)
            logger.info(f"Downsampling training data to {downsample_float:.1%} with fixed seed for reproducibility")
            rng = np.random.default_rng(1234)  # Fixed seed for consistency across runs
            n_samples = int(len(train_sequences) * downsample_float)
            indices = rng.choice(len(train_sequences), n_samples, replace=False)
            train_sequences = train_sequences[indices]
            train_experimental = train_experimental[indices]
            logger.info(f"  Training data downsampled from {len(train_sequences)/downsample_float:.0f} to {len(train_sequences)} samples")
            
            # Also downsample the ensemble predictions to match
            self.train_mean = self.train_mean[indices]
            self.train_std = self.train_std[indices]
            if self.train_aleatoric is not None:
                self.train_aleatoric = self.train_aleatoric[indices]
            logger.info(f"  Ensemble predictions downsampled to match training data")
        
        print("=== STARTING TENSOR CONVERSION ===")
        import sys
        sys.stdout.flush()
        
        # Ensure sequences match ensemble prediction sizes (should be exact after downsampling)
        if train_sequences.shape[0] != self.train_mean.shape[0]:
            logger.warning(f"Sequence count mismatch after downsampling: {train_sequences.shape[0]} vs {self.train_mean.shape[0]}")
            min_size = min(train_sequences.shape[0], self.train_mean.shape[0])
            train_sequences = train_sequences[:min_size]
            train_experimental = train_experimental[:min_size]
            self.train_mean = self.train_mean[:min_size]
            self.train_std = self.train_std[:min_size]
            if self.train_aleatoric is not None:
                self.train_aleatoric = self.train_aleatoric[:min_size]
            logger.info(f"Truncated to {min_size} samples")
            
            if val_sequences.shape[0] != self.val_mean.shape[0]:
                logger.warning(f"Validation sequence count mismatch: {val_sequences.shape[0]} vs {self.val_mean.shape[0]}")
                min_size = min(val_sequences.shape[0], self.val_mean.shape[0])
                val_sequences = val_sequences[:min_size]
                val_experimental = val_experimental[:min_size]
                self.val_mean = self.val_mean[:min_size]
                self.val_std = self.val_std[:min_size]
                if self.val_aleatoric is not None:
                    self.val_aleatoric = self.val_aleatoric[:min_size]
                logger.info(f"Truncated validation to {min_size} samples")
            
            if test_sequences.shape[0] != self.test_mean.shape[0]:
                logger.warning(f"Test sequence count mismatch: {test_sequences.shape[0]} vs {self.test_mean.shape[0]}")
                min_size = min(test_sequences.shape[0], self.test_mean.shape[0])
                test_sequences = test_sequences[:min_size]
                test_experimental = test_experimental[:min_size]
                self.test_mean = self.test_mean[:min_size]
                self.test_std = self.test_std[:min_size]
                if self.test_aleatoric is not None:
                    self.test_aleatoric = self.test_aleatoric[:min_size]
                logger.info(f"Truncated test to {min_size} samples")
        
        # Convert to tensors and transpose for DREAM-RNN format (batch, channels, length)
        # Keep on CPU to avoid memory issues, move to GPU per batch during training
        print(f"Converting sequences to tensors...")
        print(f"  Train sequences shape: {train_sequences.shape}")
        print(f"  Val sequences shape: {val_sequences.shape}")
        print(f"  Test sequences shape: {test_sequences.shape}")
        
        self.train_sequences = torch.FloatTensor(train_sequences).transpose(1, 2)  # Keep on CPU
        self.val_sequences = torch.FloatTensor(val_sequences).transpose(1, 2)      # Keep on CPU
        self.test_sequences = torch.FloatTensor(test_sequences).transpose(1, 2)    # Keep on CPU
        
        print(f"  Train sequences tensor shape: {self.train_sequences.shape}")
        print(f"  Val sequences tensor shape: {self.val_sequences.shape}")
        print(f"  Test sequences tensor shape: {self.test_sequences.shape}")
        
        # Store original experimental data for evaluation (keep on CPU)
        print(f"Converting experimental data to tensors...")
        self.train_experimental = torch.FloatTensor(train_experimental)  # Keep on CPU
        self.val_experimental = torch.FloatTensor(val_experimental)      # Keep on CPU
        self.test_experimental = torch.FloatTensor(test_experimental)    # Keep on CPU
        
        print(f"  Train experimental tensor shape: {self.train_experimental.shape}")
        print(f"  Val experimental tensor shape: {self.val_experimental.shape}")
        print(f"  Test experimental tensor shape: {self.test_experimental.shape}")
        
        print(f"All tensors created successfully and kept on CPU for per-batch GPU transfer")
        
        print(f"Loaded sequences - Train: {self.train_sequences.shape}, Val: {self.val_sequences.shape}, Test: {self.test_sequences.shape}")
        print(f"Loaded experimental data - Train: {self.train_experimental.shape}, Val: {self.val_experimental.shape}, Test: {self.test_experimental.shape}")
        
        # Force flush to ensure output appears
        import sys
        sys.stdout.flush()
        sys.stderr.flush()
    
    def _create_model(self):
        """Create DREAM-RNN model with appropriate output heads for lentiMPRA DEGU distillation."""
        print(f"=== STARTING MODEL CREATION ===")
        import sys
        sys.stdout.flush()
        
        print(f"Creating DREAM-RNN model...")
        
        # Determine number of output heads based on available data
        n_outputs = 2  # activity + epistemic uncertainty (minimum)
        if self.train_aleatoric is not None:
            n_outputs = 3  # activity + aleatoric + epistemic uncertainty
        
        print(f"  Number of output heads: {n_outputs}")
        print(f"  Input channels: 4 (A, T, G, C)")
        print(f"  Sequence length: {self.seq_len} bp")
        
        print(f"  About to create DREAM_RNN_lentiMPRA with config-driven heads...")
        sys.stdout.flush()
        
        # Create model with config-driven output heads for DEGU distillation
        # Enable epistemic uncertainty heads for distillation
        distillation_config = {
            'aleatoric': True,   # Enable aleatoric uncertainty head
            'epistemic': True,   # Enable epistemic uncertainty head
            'activation': 'relu',
            'first_activation': 'relu'
        }
        
        # Import PyTorch model from the lentiMPRA training script
        from train_DREAM_RNN_lentiMPRA import DREAM_RNN_LentiMPRA
        
        self.model = DREAM_RNN_LentiMPRA(in_channels=4, seqsize=230)
        
        # Modify final layer to have 3 outputs for unified DEGU distillation: [activity, aleatoric, epistemic]
        self.model.final_block.final_dense = nn.Linear(256, 3)
        
        print(f"  Model created, moving to device: {self.device}")
        sys.stdout.flush()
        
        self.model = self.model.to(self.device)
        
        print(f"  Model moved to device, counting parameters...")
        sys.stdout.flush()
        
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"  Model created successfully with {total_params:,} parameters")
        print(f"  Model moved to device: {self.device}")
        
        print(f"=== MODEL CREATION COMPLETE ===")
        sys.stdout.flush()
    
    def _create_data_loaders(self):
        """Create data loaders for training, validation, and test sets."""
        print(f"=== STARTING DATA LOADER CREATION ===")
        import sys
        sys.stdout.flush()
        
        print(f"Creating data loaders...")
        
        # Create datasets with sequences and targets
        if self.train_aleatoric is not None:
            print(f"  Using 3-output model (activity + aleatoric + epistemic)")
            sys.stdout.flush()
            # Include aleatoric uncertainty
            train_dataset = TensorDataset(
                self.train_sequences, 
                self.train_mean, 
                self.train_aleatoric,
                self.train_std
            )
            val_dataset = TensorDataset(
                self.val_sequences, 
                self.val_mean, 
                self.val_aleatoric,
                self.val_std
            )
            test_dataset = TensorDataset(
                self.test_sequences, 
                self.test_mean, 
                self.test_aleatoric,
                self.test_std
            )
        else:
            # Only activity and epistemic uncertainty
            train_dataset = TensorDataset(
                self.train_sequences, 
                self.train_mean, 
                self.train_std
            )
            val_dataset = TensorDataset(
                self.val_sequences, 
                self.val_mean, 
                self.val_std
            )
            test_dataset = TensorDataset(
                self.test_sequences, 
                self.test_mean, 
                self.test_std
            )
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset, 
            batch_size=self.batch_size, 
            shuffle=True,
            num_workers=0,  # Disable multi-threading to avoid CUDA issues
            pin_memory=False,
            persistent_workers=False
        )
        
        self.val_loader = DataLoader(
            val_dataset, 
            batch_size=self.batch_size, 
            shuffle=False,
            num_workers=0,  # Disable multi-threading to avoid CUDA issues
            pin_memory=False,
            persistent_workers=False
        )
        
        self.test_loader = DataLoader(
            test_dataset, 
            batch_size=self.batch_size, 
            shuffle=False,
            num_workers=0,  # Disable multi-threading to avoid CUDA issues
            pin_memory=False,
            persistent_workers=False
        )
        
        print(f"  Data loaders created successfully:")
        print(f"    Train loader: {len(self.train_loader)} batches")
        print(f"    Val loader: {len(self.val_loader)} batches")
        print(f"    Test loader: {len(self.test_loader)} batches")
        print(f"    Batch size: {self.batch_size}")
        
        print(f"Created data loaders - Train batches: {len(self.train_loader)}, Val batches: {len(self.val_loader)}, Test batches: {len(self.test_loader)}")
        
        print(f"=== DATA LOADER CREATION COMPLETE ===")
        import sys
        sys.stdout.flush()
    
    
    def _setup_training(self):
        """Set up optimizer, scheduler, and mixed precision training."""
        print(f"=== STARTING TRAINING SETUP ===")
        import sys
        sys.stdout.flush()
        
        print(f"Setting up optimizer, scheduler, and mixed precision training...")
        
        # Use AdamW optimizer as specified in DREAM paper
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=0.01,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Use OneCycleLR scheduler as in DREAM paper
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=self.learning_rate,
            epochs=self.epochs,
            steps_per_epoch=len(self.train_loader),
            pct_start=0.1,
            anneal_strategy='cos'
        )
        
        # Mixed precision training
        self.scaler = torch.cuda.amp.GradScaler()
        
        print(f"Setup training with AdamW optimizer (lr={self.learning_rate}) and OneCycleLR scheduler")
        
        print(f"=== TRAINING SETUP COMPLETE ===")
        import sys
        sys.stdout.flush()
    
    def _compute_loss(self, outputs, targets, epoch: int, batch_idx: int):
        """Compute DEGU multitask loss for lentiMPRA data."""
        if self.train_aleatoric is not None:
            # 3 outputs: activity, aleatoric uncertainty, epistemic uncertainty
            # Model outputs a single tensor with 2 values, but we need 3 for this case
            # This case shouldn't happen with current lentiMPRA setup, but keeping for completeness
            activity_pred = outputs[:, 0:1]  # First output (activity)
            aleatoric_pred = outputs[:, 1:2]  # Second output (aleatoric uncertainty)
            epistemic_pred = outputs[:, 1:2]  # Use aleatoric as epistemic proxy
            activity_target, aleatoric_target, epistemic_target = targets
            
            # Activity prediction loss (MSE)
            activity_loss = F.mse_loss(activity_pred, activity_target)
            
            # Aleatoric uncertainty loss (MSE)
            aleatoric_loss = F.mse_loss(aleatoric_pred, aleatoric_target)
            
            # Epistemic uncertainty loss (MSE)
            epistemic_loss = F.mse_loss(epistemic_pred, epistemic_target)
            
            # DEGU loss: L = L_mean + λ * L_uncertainty + γ * L_aleatoric (λ = 0, γ = 0 - no uncertainty weighting)
            lambda_uncertainty = 0.0
            gamma_aleatoric = 0.0
            total_loss = activity_loss + lambda_uncertainty * epistemic_loss + gamma_aleatoric * aleatoric_loss
            
            # Log first batch of first epoch
            if epoch == 0 and batch_idx == 0:
                print(f"Loss components - Activity: {activity_loss:.4f}, Aleatoric: {aleatoric_loss:.4f}, Epistemic: {epistemic_loss:.4f}")
            
            return total_loss, {
                'activity_loss': activity_loss.item(),
                'aleatoric_loss': aleatoric_loss.item(),
                'epistemic_loss': epistemic_loss.item()
            }
        else:
            # 2 outputs: activity, epistemic uncertainty
            # Model outputs a single tensor with 2 values, so we split it
            activity_pred = outputs[:, 0:1]  # First output (activity)
            epistemic_pred = outputs[:, 1:2]  # Second output (aleatoric uncertainty, used as epistemic proxy)
            activity_target, epistemic_target = targets
            
            # Activity prediction loss (MSE)
            activity_loss = F.mse_loss(activity_pred, activity_target)
            
            # Epistemic uncertainty loss (MSE)
            epistemic_loss = F.mse_loss(epistemic_pred, epistemic_target)
            
            # DEGU loss: L = L_mean + λ * L_uncertainty (λ = 0 - no uncertainty weighting)
            lambda_uncertainty = 0.0
            total_loss = activity_loss + lambda_uncertainty * epistemic_loss
            
            # Log first batch of first epoch
            if epoch == 0 and batch_idx == 0:
                print(f"Loss components - Activity: {activity_loss:.4f}, Epistemic: {epistemic_loss:.4f}")
            
            return total_loss, {
                'activity_loss': activity_loss.item(),
                'epistemic_loss': epistemic_loss.item()
            }
    
    def train_epoch(self, epoch: int):
        """Train for one epoch."""
        print(f"    === STARTING TRAIN_EPOCH {epoch} ===")
        import sys
        sys.stdout.flush()
        
        self.model.train()
        total_loss = 0.0
        loss_components = {}
        
        print(f"    Model set to training mode, starting batch loop...")
        sys.stdout.flush()
        
        for batch_idx, batch_data in enumerate(self.train_loader):
            if batch_idx == 0:
                print(f"    Processing first batch (batch_idx={batch_idx})...")
                sys.stdout.flush()
            if self.train_aleatoric is not None:
                sequences, activity_targets, aleatoric_targets, epistemic_targets = batch_data
                targets = (activity_targets, aleatoric_targets, epistemic_targets)
            else:
                sequences, activity_targets, epistemic_targets = batch_data
                targets = (activity_targets, epistemic_targets)
            
            # Move sequences to GPU and convert to half precision to match model weights
            sequences = sequences.to(self.device, dtype=torch.float16)
            
            # Move targets to GPU
            if self.train_aleatoric is not None:
                activity_targets, aleatoric_targets, epistemic_targets = targets
                activity_targets = activity_targets.to(self.device)
                aleatoric_targets = aleatoric_targets.to(self.device)
                epistemic_targets = epistemic_targets.to(self.device)
                targets = (activity_targets, aleatoric_targets, epistemic_targets)
            else:
                activity_targets, epistemic_targets = targets
                activity_targets = activity_targets.to(self.device)
                epistemic_targets = epistemic_targets.to(self.device)
                targets = (activity_targets, epistemic_targets)
            
            self.optimizer.zero_grad()
            
            # Mixed precision forward pass
            with torch.cuda.amp.autocast():
                outputs = self.model(sequences)
                loss, components = self._compute_loss(outputs, targets, epoch, batch_idx)
            
            # Mixed precision backward pass
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.scheduler.step()
            
            total_loss += loss.item()
            
            # Accumulate loss components
            for key, value in components.items():
                if key not in loss_components:
                    loss_components[key] = 0.0
                loss_components[key] += value
        
        # Average losses
        avg_loss = total_loss / len(self.train_loader)
        for key in loss_components:
            loss_components[key] /= len(self.train_loader)
        
        return avg_loss, loss_components
    
    def validate(self):
        """Validate against ensemble predictions (consistent with training targets)"""
        self.model.eval()
        total_loss = 0.0
        loss_components = {}
        
        with torch.no_grad():
            # Use ensemble predictions for validation (consistent with training)
            val_dataset = TensorDataset(self.val_sequences, self.val_mean, self.val_std)
            val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
            
            for sequences, mean_targets, std_targets in val_loader:
                sequences = sequences.to(self.device, dtype=torch.float16)
                mean_targets = mean_targets.to(self.device)
                std_targets = std_targets.to(self.device)
                
                with torch.cuda.amp.autocast():
                    outputs = self.model(sequences)
                    # For validation, use ensemble predictions as targets (consistent with training)
                    activity_pred = outputs[:, 0:1]  # Activity prediction
                    activity_target = mean_targets[:, 0:1]  # Ensemble mean target
                    
                    # Use MSE loss for validation (standard practice)
                    loss = nn.MSELoss()(activity_pred, activity_target)
                
                total_loss += loss.item()
                
                # Track activity loss component
                if 'activity_loss' not in loss_components:
                    loss_components['activity_loss'] = 0.0
                loss_components['activity_loss'] += loss.item()
        
        # Average losses
        avg_loss = total_loss / len(val_loader)
        for key in loss_components:
            loss_components[key] /= len(val_loader)
        
        return avg_loss, loss_components
    
    def train(self):
        """Train the DEGU distilled model."""
        start_epoch = 0
        best_val_loss = float('inf')
        
        # Load data and setup training first
        self._load_distillation_data()
        self._create_model()
        self._create_data_loaders()
        self._setup_training()
        
        print("Starting training from scratch (no resume)")
        print("Starting DEGU lentiMPRA distillation training")
        
        print("=== ABOUT TO START TRAINING LOOP ===")
        import sys
        sys.stdout.flush()
        
        # Training loop - runs for full 80 epochs with OneCycleLR (matches standard DREAM-RNN training)
        best_model_state = None
        
        for epoch in range(start_epoch, self.epochs):
            print(f"=== STARTING EPOCH {epoch} ===")
            import sys
            sys.stdout.flush()
            
            # Train
            print(f"  About to call train_epoch({epoch})...")
            sys.stdout.flush()
            
            train_loss, train_components = self.train_epoch(epoch)
            
            print(f"  train_epoch({epoch}) completed, loss: {train_loss:.4f}")
            sys.stdout.flush()
            
            # Validate
            val_loss, val_components = self.validate()
            
            # Print progress (matching DREAM-RNN format)
            print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                  f"Val Activity Loss: {val_components['activity_loss']:.4f}")
            
            # Save best model based on validation loss (following standard DREAM-RNN methodology)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_state = self.model.state_dict().copy()
                print(f"New best validation loss: {best_val_loss:.4f}")
            
            # Save model checkpoint every 10 epochs (like standard training)
            if (epoch + 1) % 10 == 0:
                checkpoint_path = self.output_dir / f"{self.model_index}_degu_model_epoch_{epoch+1}.pth"
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'best_val_loss': best_val_loss
                }
                torch.save(checkpoint, checkpoint_path)
                print(f"Saved checkpoint to: {checkpoint_path}")
        
        print(f"Training completed after {self.epochs} epochs")
        
        # Load best model (following standard DREAM-RNN methodology)
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            print(f"Loaded best model with validation loss: {best_val_loss:.4f}")
        
        # Save final model checkpoint (best model)
        final_model_path = self.output_dir / f"{self.model_index}_degu_model_final.pth"
        torch.save(self.model.state_dict(), final_model_path)
        print(f"Saved final model to: {final_model_path}")
        
        # Evaluate the final model and save performance results
        self._evaluate_and_save_performance()

    def _evaluate_and_save_performance(self):
        """Evaluate the trained model and save performance results in plotting-compatible format"""
        print("Evaluating model performance...")
        
        # Load the final model checkpoint
        final_model_path = self.output_dir / f"{self.model_index}_degu_model_final.pth"
        if final_model_path.exists():
            self.model.load_state_dict(torch.load(final_model_path, map_location=self.device))
            print(f"Loaded final model from: {final_model_path}")
        else:
            # Fallback to latest epoch checkpoint
            checkpoint_files = list(self.output_dir.glob(f"{self.model_index}_degu_model_epoch_*.pth"))
            if checkpoint_files:
                # Sort by epoch number and get the latest
                latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.stem.split('_')[-1]))
                self.model.load_state_dict(torch.load(latest_checkpoint, map_location=self.device))
                print(f"Loaded latest model from: {latest_checkpoint}")
            else:
                # Fallback to old naming convention
                best_model_path = self.output_dir / f"{self.model_index}_degu_model.pth"
                if best_model_path.exists():
                    self.model.load_state_dict(torch.load(best_model_path, map_location=self.device))
                    print(f"Loaded best model from: {best_model_path}")
                else:
                    print("Warning: No model checkpoint found for evaluation!")
        
        self.model.eval()
        
        # Evaluate on test set using ORIGINAL EXPERIMENTAL DATA (not ensemble predictions)
        test_predictions = []
        test_targets = []
        
        with torch.no_grad():
            # Create a simple test loader with just sequences and original experimental data
            test_dataset = TensorDataset(self.test_sequences, self.test_experimental)
            test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
            
            for sequences, experimental_targets in test_loader:
                sequences = sequences.to(self.device)
                experimental_targets = experimental_targets.to(self.device)
                
                outputs = self.model(sequences)
                
                # Handle different output formats
                if self.train_aleatoric is not None:
                    # 3 outputs: activity, aleatoric, epistemic
                    activity_pred = outputs[:, 0:1]
                    aleatoric_pred = outputs[:, 1:2]
                    epistemic_pred = outputs[:, 2:3]
                    test_predictions.append(activity_pred.cpu())
                    test_targets.append(experimental_targets[:, 0:1].cpu())  # Original experimental activity
                else:
                    # 2 outputs: activity, epistemic uncertainty
                    activity_pred = outputs[:, 0:1]
                    epistemic_pred = outputs[:, 1:2]
                    test_predictions.append(activity_pred.cpu())
                    test_targets.append(experimental_targets[:, 0:1].cpu())  # Original experimental activity
        
        # Concatenate all predictions and targets
        test_predictions = torch.cat(test_predictions, dim=0).numpy()
        test_targets = torch.cat(test_targets, dim=0).numpy()
        
        # Calculate performance metrics against ORIGINAL EXPERIMENTAL DATA
        from scipy.stats import pearsonr, spearmanr
        
        # Flatten arrays for correlation calculations
        pred_flat = test_predictions.flatten()
        target_flat = test_targets.flatten()
        
        # Calculate Pearson and Spearman correlations
        pearson_r, _ = pearsonr(pred_flat, target_flat)
        spearman_rho, _ = spearmanr(pred_flat, target_flat)
        
        # Calculate MSE
        mse = np.mean((pred_flat - target_flat) ** 2)
        
        print(f"Evaluation against ORIGINAL EXPERIMENTAL DATA:")
        print(f"  Test samples: {len(pred_flat)}")
        print(f"  Activity Pearson: {pearson_r:.6f}")
        print(f"  Activity Spearman: {spearman_rho:.6f}")
        print(f"  Activity MSE: {mse:.6f}")
        
        print(f"Test Performance:")
        print(f"  Pearson r: {pearson_r:.4f}")
        print(f"  Spearman ρ: {spearman_rho:.4f}")
        print(f"  MSE: {mse:.6f}")
        
        # Save performance results in plotting-compatible format
        performance_data = {
            'Dev_pearson': pearson_r,  # For DeepSTARR Dev
            'Hk_pearson': pearson_r,   # For DeepSTARR Hk (same model, different interpretation)
            'activity_pearson': pearson_r,  # For lentiMPRA
            'Dev_spearman': spearman_rho,
            'Hk_spearman': spearman_rho,
            'activity_spearman': spearman_rho,
            'Dev_mse': mse,
            'Hk_mse': mse,
            'activity_mse': mse,
            'n_test_samples': len(test_targets)
        }
        
        # Save as CSV
        performance_df = pd.DataFrame([performance_data])
        performance_path = self.output_dir / f"{self.model_index}_performance.csv"
        performance_df.to_csv(performance_path, index=False)
        print(f"Saved performance results to: {performance_path}")

def main():
    parser = argparse.ArgumentParser(description='DEGU Distillation Training for lentiMPRA DREAM-RNN Models')
    
    parser.add_argument('--distillation_data', type=str, required=True,
                       help='Path to .npz file containing ensemble predictions')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='Directory to save trained model')
    parser.add_argument('--model_index', type=int, default=0,
                       help='Model index for random seed (default: 0)')
    parser.add_argument('--batch_size', type=int, default=1024,
                       help='Batch size for training (default: 1024)')
    parser.add_argument('--learning_rate', type=float, default=0.005,
                       help='Learning rate (default: 0.005)')
    parser.add_argument('--epochs', type=int, default=80,
                       help='Number of training epochs (default: 80)')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (default: cuda)')
    parser.add_argument('--config', type=str, default='paper_reproducibility/config/DREAM_RNN_lentiMPRA.yaml',
                       help='Config file (default: DREAM_RNN_lentiMPRA.yaml)')
    
    args = parser.parse_args()
    
    # Load config
    import yaml
    with open(args.config, 'r') as f:
        config_raw = yaml.safe_load(f)
    
    config = {}
    for key, item in config_raw.items():
        if isinstance(item, dict) and 'value' in item:
            config[key] = item['value']
        else:
            config[key] = item
    
    # Disable cuDNN to avoid version conflicts
    torch.backends.cudnn.enabled = False
    print("Disabled cuDNN to avoid version conflicts")
    
    # Create trainer and start training
    trainer = DEGULentiMPRADistillationTrainer(
        distillation_data_path=args.distillation_data,
        output_dir=args.output_dir,
        model_index=args.model_index,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        epochs=args.epochs,
        device=args.device,
        config=config
    )
    
    trainer.train()

if __name__ == '__main__':
    main()
