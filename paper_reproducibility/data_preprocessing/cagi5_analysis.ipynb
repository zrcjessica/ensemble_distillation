{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse CAGI5 data to get WT and mutant sequences for zero-shot prediction with ResidualBind models.\n",
    "\n",
    "Includes code to identify and add in missing alternate alleles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, h5py\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_const_range(site, window):\n",
    "    \"\"\"\n",
    "    Function to get constant size bed ranges\n",
    "    :param site: center positions\n",
    "    :param window: window size around center position\n",
    "    :return: new starts and ends\n",
    "    \"\"\"\n",
    "    half_window = window//2\n",
    "    start = site - half_window\n",
    "    end = site + half_window\n",
    "    return start, end\n",
    "\n",
    "def expand_range(bedfile, output_filename, window=3072):\n",
    "    \"\"\"\n",
    "    Function to write a new bed file with expanded ranges\n",
    "    :param bedfile: existing bed file, the ranges of which will be expanded\n",
    "    :param output_filename: new bed file path\n",
    "    :param window: window size\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df = pd.read_table(bedfile, header=None, index_col=None)\n",
    "    start, end = enforce_const_range(df.iloc[:,1].astype(int), window)\n",
    "    df_expanded = df.copy()\n",
    "    df_expanded.iloc[:,1] = start.values\n",
    "    df_expanded.iloc[:,2] = end.values\n",
    "    df_nonneg = df_expanded[df_expanded.iloc[:,1]>0]\n",
    "    df_nonneg = df_nonneg.reset_index(drop=True)\n",
    "    # add string identifier (chr:start-end/ref>alt)\n",
    "    df_nonneg['identifier'] = [f'{df_nonneg.iloc[i,0]}:{df_nonneg.iloc[i,1]}-{df_nonneg.iloc[i,2]}/{df_nonneg.iloc[i,3]}>{df_nonneg.iloc[i,4]}' for i in range(df_nonneg.shape[0])] \n",
    "    df_nonneg.to_csv(output_filename, header=None, sep='\\t', index=None)\n",
    "\n",
    "def convert_bed_to_seq(bedfile, output_fa, genomefile):\n",
    "    \"\"\"\n",
    "    This function collects DNA sequences corresponding to a bedfile into a fasta file\n",
    "    :param bedfile: existing bed file\n",
    "    :param output_fa: new fasta path (generated if it doesn't exist)\n",
    "    :param genomefile: genome fasta to use to get the sequences\n",
    "    :return: list of coordinates and string sequences\n",
    "    \"\"\"\n",
    "    if os.path.isfile(output_fa) is not True:\n",
    "        '''\n",
    "        generate fasta file if it doesn't exist yet \n",
    "        '''\n",
    "        cmd = 'bedtools getfasta -fi {} -bed {} -s -fo {}'.format(genomefile, bedfile, output_fa)\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
    "        _ = process.communicate()\n",
    "    coords_list, seqs_list = fasta2list(output_fa)\n",
    "    return coords_list, seqs_list\n",
    "\n",
    "\n",
    "def fasta2list(fasta_file):\n",
    "    \"\"\"\n",
    "    Function to convert fasta file to a list of DNA strings\n",
    "    :param fasta_file: existing fasta file\n",
    "    :return: list of coordinates and string sequences\n",
    "    \"\"\"\n",
    "    fasta_coords = []\n",
    "    seqs = []\n",
    "    # header = ''\n",
    "\n",
    "    for line in open(fasta_file):\n",
    "        if line[0] == '>':\n",
    "            # header = line.split()[0][1:]\n",
    "            fasta_coords.append(line[1:].rstrip())\n",
    "        else:\n",
    "            s = line.rstrip()\n",
    "            s = s.upper()\n",
    "            seqs.append(s)\n",
    "\n",
    "    return fasta_coords, seqs\n",
    "\n",
    "def dna_one_hot(seq):\n",
    "    \"\"\"\n",
    "    Function to convert string DNA sequences into onehot\n",
    "    :param seq: string DNA sequence\n",
    "    :return: onehot sequence\n",
    "    \"\"\"\n",
    "    seq_len = len(seq)\n",
    "    seq_start = 0\n",
    "    seq = seq.upper()\n",
    "\n",
    "    # map nt's to a matrix len(seq)x4 of 0's and 1's.\n",
    "    seq_code = np.zeros((seq_len, 4), dtype='float16')\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        if i >= seq_start and i - seq_start < len(seq):\n",
    "            nt = seq[i - seq_start]\n",
    "            if nt == 'A':\n",
    "                seq_code[i, 0] = 1\n",
    "            elif nt == 'C':\n",
    "                seq_code[i, 1] = 1\n",
    "            elif nt == 'G':\n",
    "                seq_code[i, 2] = 1\n",
    "            elif nt == 'T':\n",
    "                seq_code[i, 3] = 1\n",
    "            else:\n",
    "                seq_code[i, :] = 0.25\n",
    "\n",
    "    return seq_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse combined BED files from CAGI5 challenge + release data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regulator: IRF6\n",
      "writing combined bed file to ../data/cagi5/IRF6_combined_cagi.bed\n",
      "regulator: MYCrs6983267\n",
      "writing combined bed file to ../data/cagi5/MYCrs6983267_combined_cagi.bed\n",
      "regulator: F9\n",
      "writing combined bed file to ../data/cagi5/F9_combined_cagi.bed\n",
      "regulator: PKLR\n",
      "writing combined bed file to ../data/cagi5/PKLR_combined_cagi.bed\n",
      "regulator: GP1BB\n",
      "writing combined bed file to ../data/cagi5/GP1BB_combined_cagi.bed\n",
      "regulator: HNF4A\n",
      "writing combined bed file to ../data/cagi5/HNF4A_combined_cagi.bed\n",
      "regulator: IRF4\n",
      "writing combined bed file to ../data/cagi5/IRF4_combined_cagi.bed\n",
      "regulator: ZFAND3\n",
      "writing combined bed file to ../data/cagi5/ZFAND3_combined_cagi.bed\n",
      "regulator: TERT-HEK293T\n",
      "writing combined bed file to ../data/cagi5/TERT-HEK293T_combined_cagi.bed\n",
      "regulator: MSMB\n",
      "writing combined bed file to ../data/cagi5/MSMB_combined_cagi.bed\n",
      "regulator: HBG1\n",
      "writing combined bed file to ../data/cagi5/HBG1_combined_cagi.bed\n",
      "regulator: LDLR\n",
      "writing combined bed file to ../data/cagi5/LDLR_combined_cagi.bed\n",
      "regulator: TERT-GBM\n",
      "writing combined bed file to ../data/cagi5/TERT-GBM_combined_cagi.bed\n",
      "regulator: HBB\n",
      "writing combined bed file to ../data/cagi5/HBB_combined_cagi.bed\n",
      "regulator: SORT1\n",
      "writing combined bed file to ../data/cagi5/SORT1_combined_cagi.bed\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for each enhancer/promoter (regulator)\n",
    "combine challenge and release data as a single bed file \n",
    "'''\n",
    "\n",
    "all_dfs = defaultdict(list)\n",
    "cagi_data = '../data/cagi5/'\n",
    "combined_filename = '../data/cagi5/combined_cagi.bed'\n",
    "for filename in glob.glob('../data/cagi5/*tsv'):\n",
    "    prefix, regulator = os.path.basename(filename).split('.tsv')[0].split('_')\n",
    "    one_reg = pd.read_table(filename, skiprows=7, header=None)\n",
    "    one_reg['regulator'] = regulator\n",
    "    one_reg['set'] = prefix\n",
    "    all_dfs[regulator].append(one_reg)\n",
    "    \n",
    "for regulator, dfs in all_dfs.items():\n",
    "    print(f'regulator: {regulator}')\n",
    "    combined_filename = f'../data/cagi5/{regulator}_combined_cagi.bed'\n",
    "    combined_cagi = pd.concat(dfs)\n",
    "    combined_cagi.insert(4, 'strand', '+')\n",
    "    combined_cagi.insert(2,'end',combined_cagi.iloc[:,1]+1)\n",
    "    combined_cagi.iloc[:,0] = 'chr'+combined_cagi.iloc[:,0].astype(str)\n",
    "    combined_cagi.iloc[:,1] = combined_cagi.iloc[:,1].astype(int)\n",
    "    combined_cagi.iloc[:,2] = combined_cagi.iloc[:,1].astype(int)\n",
    "    print(f'writing combined bed file to {combined_filename}')\n",
    "    combined_cagi.to_csv(combined_filename, sep='\\t', header=False, index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define seqlen for ResidualBind lentiMPRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input length for ResidualBind lentiMPRA models\n",
    "seqlen = 230 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K562\n",
    "## _PKLR_\n",
    "### Generate BED file w/ expanded ranges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulator = 'PKLR'\n",
    "\n",
    "# output filename for expanded bed file\n",
    "output_filename = f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp.bed'\n",
    "\n",
    "if os.path.isfile(output_filename) is not True:\n",
    "    # generate bed file with expanded range \n",
    "    expand_range(f'../data/cagi5/{regulator}_combined_cagi.bed', output_filename, window = seqlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sequences and their coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_filename = f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp.fa'\n",
    "coords_list, seqs_list = convert_bed_to_seq(output_filename, fa_filename, genomefile='/shared/jessica/ref/hg19/hg19.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate one hot seqs (WT & Mutant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lines = []\n",
    "nonneg_df = pd.read_csv(output_filename, sep='\\t', header=None) # load bed file with ref/alt alleles \n",
    "onehot_ref = []\n",
    "onehot_alt = []\n",
    "identifiers = []\n",
    "pos_dict = {'+': int(seqlen/2-1), '-':int(seqlen/2)} # ix of allele pos in seq\n",
    "for i,(chr_s_e, seq) in enumerate(zip(coords_list, seqs_list)):\n",
    "    alt = ''\n",
    "    strand = chr_s_e.split('(')[-1].split(')')[0]\n",
    "    pos = pos_dict[strand] # get allele pos \n",
    "    if seq[pos] != nonneg_df.iloc[i, 3]:\n",
    "        # check that nucleotide at pos is ref allele \n",
    "        print('Error in line ' + str(i))\n",
    "        bad_lines.append(i)\n",
    "    else:\n",
    "        # get alt allele \n",
    "        alt = nonneg_df.iloc[i,4]\n",
    "        onehot = dna_one_hot(seq) # one hot wt seq \n",
    "        mutated_onehot = onehot.copy() \n",
    "        mutated_onehot[pos] = dna_one_hot(alt)[0] # one hot mutant seq \n",
    "        onehot_ref.append(onehot)\n",
    "        onehot_alt.append(mutated_onehot) \n",
    "        identifiers.append(nonneg_df.iloc[i,-1])\n",
    "\n",
    "onehot_alt = np.array(onehot_alt)\n",
    "onehot_ref = np.array(onehot_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove badlines \n",
    "included_df = nonneg_df[~nonneg_df.index.isin(bad_lines)]\n",
    "included_df.to_csv(f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp_filt.bed', header=None, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write onehot seqs and identifiers to .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_seqs_fh = f'../data/cagi5/{regulator}_onehot_{seqlen}bp.h5'\n",
    "with h5py.File(onehot_seqs_fh, 'w') as fh:\n",
    "    fh.create_dataset('ref', data=onehot_ref)\n",
    "    fh.create_dataset('alt', data=onehot_alt)\n",
    "    fh.create_dataset('identifier', data=identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine h5 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(onehot_seqs_fh, 'r') as fh:\n",
    "    ref_seqs = np.array(fh['ref'])\n",
    "    alt_seqs = np.array(fh['alt'])\n",
    "    seq_identifiers = list(fh['identifier'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409, 230, 4)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409, 230, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_identifiers = [x.decode('utf-8') for x in seq_identifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in missing alternate alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>155271119</td>\n",
       "      <td>155271349</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>+</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PKLR</td>\n",
       "      <td>release</td>\n",
       "      <td>chr1:155271119-155271349/A&gt;C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>155271119</td>\n",
       "      <td>155271349</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>+</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.85</td>\n",
       "      <td>PKLR</td>\n",
       "      <td>release</td>\n",
       "      <td>chr1:155271119-155271349/A&gt;G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>155271119</td>\n",
       "      <td>155271349</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>+</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.04</td>\n",
       "      <td>PKLR</td>\n",
       "      <td>release</td>\n",
       "      <td>chr1:155271119-155271349/A&gt;T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>155271120</td>\n",
       "      <td>155271350</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>+</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>PKLR</td>\n",
       "      <td>release</td>\n",
       "      <td>chr1:155271120-155271350/G&gt;A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>155271120</td>\n",
       "      <td>155271350</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>+</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PKLR</td>\n",
       "      <td>release</td>\n",
       "      <td>chr1:155271120-155271350/G&gt;C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0          1          2  3  4  5     6     7     8        9   \\\n",
       "0  chr1  155271119  155271349  A  C  +  0.07  0.00  PKLR  release   \n",
       "1  chr1  155271119  155271349  A  G  + -0.67  0.85  PKLR  release   \n",
       "2  chr1  155271119  155271349  A  T  +  0.19  0.04  PKLR  release   \n",
       "3  chr1  155271120  155271350  G  A  + -0.10  0.01  PKLR  release   \n",
       "4  chr1  155271120  155271350  G  C  +  0.06  0.00  PKLR  release   \n",
       "\n",
       "                             10  \n",
       "0  chr1:155271119-155271349/A>C  \n",
       "1  chr1:155271119-155271349/A>G  \n",
       "2  chr1:155271119-155271349/A>T  \n",
       "3  chr1:155271120-155271350/G>A  \n",
       "4  chr1:155271120-155271350/G>C  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pklr_bed = pd.read_table(output_filename, header=None)\n",
    "pklr_bed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1          2          3\n",
       "chr1  155271142  155271372  A    2\n",
       "      155271251  155271481  C    2\n",
       "      155271537  155271767  A    2\n",
       "      155271540  155271770  T    2\n",
       "Name: 4, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify missing alleles\n",
    "result = pklr_bed.groupby([0,1,2,3])[4].nunique()\n",
    "cases_with_missing_alleles = result[result!=3]\n",
    "cases_with_missing_alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0          1          2  3  4  5   6   7     8        9   \\\n",
      "0  chr1  155271142  155271372  A  C  +  NA  NA  PKLR  missing   \n",
      "\n",
      "                             10  \n",
      "0  chr1:155271142-155271372/A/C  \n",
      "inserting new row with missing alt allele C at ix 551\n",
      "     0          1          2  3  4  5   6   7     8        9   \\\n",
      "0  chr1  155271251  155271481  C  G  +  NA  NA  PKLR  missing   \n",
      "\n",
      "                             10  \n",
      "0  chr1:155271251-155271481/C/G  \n",
      "inserting new row with missing alt allele G at ix 782\n",
      "     0          1          2  3  4  5   6   7     8        9   \\\n",
      "0  chr1  155271537  155271767  A  C  +  NA  NA  PKLR  missing   \n",
      "\n",
      "                             10  \n",
      "0  chr1:155271537-155271767/A/C  \n",
      "inserting new row with missing alt allele C at ix 1400\n",
      "     0          1          2  3  4  5   6   7     8        9   \\\n",
      "0  chr1  155271540  155271770  T  G  +  NA  NA  PKLR  missing   \n",
      "\n",
      "                             10  \n",
      "0  chr1:155271540-155271770/T/G  \n",
      "inserting new row with missing alt allele G at ix 1409\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cases_with_missing_alleles)):\n",
    "    chrom, start, end, ref = cases_with_missing_alleles.index[i]\n",
    "    sub_df = pklr_bed[(pklr_bed.iloc[:,0]==chrom) & (pklr_bed.iloc[:,1]==start)]\n",
    "    alt = sub_df.iloc[:,4].tolist()\n",
    "    alleles = set(alt + [ref])\n",
    "    missing = (set(['A','C','G','T']) - alleles).pop()\n",
    "    new_row = pd.DataFrame([chrom, start, end, ref, missing, '+', 'NA', 'NA', 'PKLR','missing', f'{chrom}:{start}-{end}/{ref}/{missing}']).transpose()\n",
    "    print(new_row)\n",
    "    loc = pklr_bed[(pklr_bed.iloc[:,0]==cases_with_missing_alleles.index[i][0]) & (pklr_bed.iloc[:,1]==cases_with_missing_alleles.index[i][1])].index[-1] + 1\n",
    "    # insert new row\n",
    "    print(f'inserting new row with missing alt allele {missing} at ix {loc}')\n",
    "    pklr_bed = pd.concat([pklr_bed.iloc[:loc,:],\n",
    "                          new_row,\n",
    "                          pklr_bed.iloc[loc:,:]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file \n",
    "pklr_bed.to_csv(f'../data/cagi5/{regulator}_combined_cagi_with_missing_alt_alleles_230bp.bed', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sequences and their coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_filename = f'../data/cagi5/{regulator}_cagi_with_missing_alt_alleles_{seqlen}bp.fa'\n",
    "coords_list, seqs_list = convert_bed_to_seq('../data/cagi5/{regulator}_combined_cagi_with_missing_alt_alleles_230bp.bed', \n",
    "                                            fa_filename, genomefile='/shared/jessica/ref/hg19/hg19.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate one hot seqs (WT & Mutant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lines = []\n",
    "nonneg_df = pd.read_csv(f'../data/cagi5/{regulator}_combined_cagi_with_missing_alt_alleles_230bp.bed', sep='\\t', header=None) # load bed file with ref/alt alleles \n",
    "onehot_ref = []\n",
    "onehot_alt = []\n",
    "identifiers = []\n",
    "pos_dict = {'+': int(seqlen/2-1), '-':int(seqlen/2)} # ix of allele pos in seq\n",
    "for i,(chr_s_e, seq) in enumerate(zip(coords_list, seqs_list)):\n",
    "    alt = ''\n",
    "    strand = chr_s_e.split('(')[-1].split(')')[0]\n",
    "    pos = pos_dict[strand] # get allele pos \n",
    "    if seq[pos] != nonneg_df.iloc[i, 3]:\n",
    "        # check that nucleotide at pos is ref allele \n",
    "        print('Error in line ' + str(i))\n",
    "        bad_lines.append(i)\n",
    "    else:\n",
    "        # get alt allele \n",
    "        alt = nonneg_df.iloc[i,4]\n",
    "        onehot = dna_one_hot(seq) # one hot wt seq \n",
    "        mutated_onehot = onehot.copy() \n",
    "        mutated_onehot[pos] = dna_one_hot(alt)[0] # one hot mutant seq \n",
    "        onehot_ref.append(onehot)\n",
    "        onehot_alt.append(mutated_onehot) \n",
    "        identifiers.append(nonneg_df.iloc[i,-1])\n",
    "\n",
    "onehot_alt = np.array(onehot_alt)\n",
    "onehot_ref = np.array(onehot_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove badlines \n",
    "included_df = nonneg_df[~nonneg_df.index.isin(bad_lines)]\n",
    "included_df.to_csv(f'../data/cagi5/{regulator}_combined_cagi_with_missing_alt_alleles_{seqlen}bp_filt.bed', header=None, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write onehot seqs and identifiers to .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_seqs_fh = f'../data/cagi5/{regulator}_with_missing_alt_alleles_onehot_{seqlen}bp.h5'\n",
    "with h5py.File(onehot_seqs_fh, 'w') as fh:\n",
    "    fh.create_dataset('ref', data=onehot_ref)\n",
    "    fh.create_dataset('alt', data=onehot_alt)\n",
    "    fh.create_dataset('identifier', data=identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HepG2\n",
    "- _F9_\n",
    "- _LDLR_\n",
    "- _SORT1_\n",
    "## _F9_\n",
    "\n",
    "### Generate BED file w/ expanded ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulator = 'F9'\n",
    "\n",
    "# output filename for expanded bed file\n",
    "output_filename = f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp.bed'\n",
    "\n",
    "if os.path.isfile(output_filename) is not True:\n",
    "    # generate bed file with expanded range \n",
    "    expand_range(f'../data/cagi5/{regulator}_combined_cagi.bed', output_filename, window = seqlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sequences and their coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_filename = f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp.fa'\n",
    "coords_list, seqs_list = convert_bed_to_seq(output_filename, fa_filename, genomefile='/shared/jessica/ref/hg19/hg19.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate one hot seqs (WT & Mutant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in line 0\n",
      "Error in line 1\n",
      "Error in line 2\n"
     ]
    }
   ],
   "source": [
    "bad_lines = []\n",
    "nonneg_df = pd.read_csv(output_filename, sep='\\t', header=None) # load bed file with ref/alt alleles \n",
    "onehot_ref = []\n",
    "onehot_alt = []\n",
    "identifiers = []\n",
    "pos_dict = {'+': int(seqlen/2-1), '-':int(seqlen/2)} # ix of allele pos in seq\n",
    "for i,(chr_s_e, seq) in enumerate(zip(coords_list, seqs_list)):\n",
    "    alt = ''\n",
    "    strand = chr_s_e.split('(')[-1].split(')')[0]\n",
    "    pos = pos_dict[strand] # get allele pos \n",
    "    if seq[pos] != nonneg_df.iloc[i, 3]:\n",
    "        # check that nucleotide at pos is ref allele \n",
    "        print('Error in line ' + str(i))\n",
    "        bad_lines.append(i)\n",
    "    else:\n",
    "        # get alt allele \n",
    "        alt = nonneg_df.iloc[i,4]\n",
    "        onehot = dna_one_hot(seq) # one hot wt seq \n",
    "        mutated_onehot = onehot.copy() \n",
    "        mutated_onehot[pos] = dna_one_hot(alt)[0] # one hot mutant seq \n",
    "        onehot_ref.append(onehot)\n",
    "        onehot_alt.append(mutated_onehot) \n",
    "        identifiers.append(nonneg_df.iloc[i,-1])\n",
    "\n",
    "onehot_alt = np.array(onehot_alt)\n",
    "onehot_ref = np.array(onehot_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove badlines \n",
    "included_df = nonneg_df[~nonneg_df.index.isin(bad_lines)]\n",
    "included_df.to_csv(f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp_filt.bed', header=None, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write onehot seqs and identifiers to .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_seqs_fh = f'../data/cagi5/{regulator}_onehot_{seqlen}bp.h5'\n",
    "with h5py.File(onehot_seqs_fh, 'w') as fh:\n",
    "    fh.create_dataset('ref', data=onehot_ref)\n",
    "    fh.create_dataset('alt', data=onehot_alt)\n",
    "    fh.create_dataset('identifier', data=identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _LDLR_\n",
    "\n",
    "### Generate BED file w/ expanded ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulator = 'LDLR'\n",
    "\n",
    "# output filename for expanded bed file\n",
    "output_filename = f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp.bed'\n",
    "\n",
    "if os.path.isfile(output_filename) is not True:\n",
    "    # generate bed file with expanded range \n",
    "    expand_range(f'../data/cagi5/{regulator}_combined_cagi.bed', output_filename, window = seqlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sequences and their coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_filename = f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp.fa'\n",
    "coords_list, seqs_list = convert_bed_to_seq(output_filename, fa_filename, genomefile='/shared/jessica/ref/hg19/hg19.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate one hot seqs (WT & Mutant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lines = []\n",
    "nonneg_df = pd.read_csv(output_filename, sep='\\t', header=None) # load bed file with ref/alt alleles \n",
    "onehot_ref = []\n",
    "onehot_alt = []\n",
    "identifiers = []\n",
    "pos_dict = {'+': int(seqlen/2-1), '-':int(seqlen/2)} # ix of allele pos in seq\n",
    "for i,(chr_s_e, seq) in enumerate(zip(coords_list, seqs_list)):\n",
    "    alt = ''\n",
    "    strand = chr_s_e.split('(')[-1].split(')')[0]\n",
    "    pos = pos_dict[strand] # get allele pos \n",
    "    if seq[pos] != nonneg_df.iloc[i, 3]:\n",
    "        # check that nucleotide at pos is ref allele \n",
    "        print('Error in line ' + str(i))\n",
    "        bad_lines.append(i)\n",
    "    else:\n",
    "        # get alt allele \n",
    "        alt = nonneg_df.iloc[i,4]\n",
    "        onehot = dna_one_hot(seq) # one hot wt seq \n",
    "        mutated_onehot = onehot.copy() \n",
    "        mutated_onehot[pos] = dna_one_hot(alt)[0] # one hot mutant seq \n",
    "        onehot_ref.append(onehot)\n",
    "        onehot_alt.append(mutated_onehot) \n",
    "        identifiers.append(nonneg_df.iloc[i,-1])\n",
    "\n",
    "onehot_alt = np.array(onehot_alt)\n",
    "onehot_ref = np.array(onehot_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove badlines \n",
    "included_df = nonneg_df[~nonneg_df.index.isin(bad_lines)]\n",
    "included_df.to_csv(f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp_filt.bed', header=None, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write onehot seqs and identifiers to .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_seqs_fh = f'../data/cagi5/{regulator}_onehot_{seqlen}bp.h5'\n",
    "with h5py.File(onehot_seqs_fh, 'w') as fh:\n",
    "    fh.create_dataset('ref', data=onehot_ref)\n",
    "    fh.create_dataset('alt', data=onehot_alt)\n",
    "    fh.create_dataset('identifier', data=identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _SORT1_\n",
    "\n",
    "### Generate BED file w/ expanded ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulator = 'SORT1'\n",
    "\n",
    "# output filename for expanded bed file\n",
    "output_filename = f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp.bed'\n",
    "\n",
    "if os.path.isfile(output_filename) is not True:\n",
    "    # generate bed file with expanded range \n",
    "    expand_range(f'../data/cagi5/{regulator}_combined_cagi.bed', output_filename, window = seqlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sequences and their coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_filename = f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp.fa'\n",
    "coords_list, seqs_list = convert_bed_to_seq(output_filename, fa_filename, genomefile='/shared/jessica/ref/hg19/hg19.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate one hot seqs (WT & Mutant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in line 0\n",
      "Error in line 1\n"
     ]
    }
   ],
   "source": [
    "bad_lines = []\n",
    "nonneg_df = pd.read_csv(output_filename, sep='\\t', header=None) # load bed file with ref/alt alleles \n",
    "onehot_ref = []\n",
    "onehot_alt = []\n",
    "identifiers = []\n",
    "pos_dict = {'+': int(seqlen/2-1), '-':int(seqlen/2)} # ix of allele pos in seq\n",
    "for i,(chr_s_e, seq) in enumerate(zip(coords_list, seqs_list)):\n",
    "    alt = ''\n",
    "    strand = chr_s_e.split('(')[-1].split(')')[0]\n",
    "    pos = pos_dict[strand] # get allele pos \n",
    "    if seq[pos] != nonneg_df.iloc[i, 3]:\n",
    "        # check that nucleotide at pos is ref allele \n",
    "        print('Error in line ' + str(i))\n",
    "        bad_lines.append(i)\n",
    "    else:\n",
    "        # get alt allele \n",
    "        alt = nonneg_df.iloc[i,4]\n",
    "        onehot = dna_one_hot(seq) # one hot wt seq \n",
    "        mutated_onehot = onehot.copy() \n",
    "        mutated_onehot[pos] = dna_one_hot(alt)[0] # one hot mutant seq \n",
    "        onehot_ref.append(onehot)\n",
    "        onehot_alt.append(mutated_onehot) \n",
    "        identifiers.append(nonneg_df.iloc[i,-1])\n",
    "\n",
    "onehot_alt = np.array(onehot_alt)\n",
    "onehot_ref = np.array(onehot_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove badlines \n",
    "included_df = nonneg_df[~nonneg_df.index.isin(bad_lines)]\n",
    "included_df.to_csv(f'../data/cagi5/{regulator}_combined_cagi_{seqlen}bp_filt.bed', header=None, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write onehot seqs and identifiers to .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_seqs_fh = f'../data/cagi5/{regulator}_onehot_{seqlen}bp.h5'\n",
    "with h5py.File(onehot_seqs_fh, 'w') as fh:\n",
    "    fh.create_dataset('ref', data=onehot_ref)\n",
    "    fh.create_dataset('alt', data=onehot_alt)\n",
    "    fh.create_dataset('identifier', data=identifiers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
